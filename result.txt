Params:
suffix_length = 10
nb_steps = 10000
lr = 10

Outputs:
(venv) membit@coktailjet:~/projects/reverse/reverse_engineering_llms$ python3 continuous_descent.py
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
batch [1524, 19, 8, 1784, 13, 32099, 3, 5]
Initial prompt tokens: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']
Embedding(32100, 768)
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels tokens: ['<extra_id_0>', '▁France', '<extra_id_1>']
100%|████████████████████████████████████████████████| 10000/10000 [41:56<00:00,  3.97it/s]
prompt_ids[0] tensor([    5,     5,     5,     5,     5,     5,     5,     5,     5,     5,
         1524,    19,     8,  1784,    13, 32099,     3,     5,     1],
       device='cuda:0')
Optimized prompt: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
the United Kingdom the United Kingdom the United Kingdom the United Kingdom the United

--------Generated with the optimized prompt--------
the United Kingdom. England England England... the United Kingdom
(venv) membit@coktailjet:~/projects/reverse/reverse_engineering_llms$


Params:
suffix_length = 10
nb_steps = 10000
lr = 1000

Result:
(venv) membit@coktailjet:~/projects/reverse/reverse_engineering_llms$ python3 continuous_descent.py
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
batch [1524, 19, 8, 1784, 13, 32099, 3, 5]
Initial prompt tokens: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']
Embedding(32100, 768)
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels tokens: ['<extra_id_0>', '▁France', '<extra_id_1>']
100%|████████████████████████████████████████████████| 10000/10000 [45:15<00:00,  3.68it/s]
prompt_ids[0] tensor([    5,     5,     5,     5,     5,     5,     5,     5,     5,     5,
         1524,    19,     8,  1784,    13, 32099,     3,     5,     1],
       device='cuda:0')
Optimized prompt: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
the United Kingdom the United Kingdom the United Kingdom the United Kingdom the United

--------Generated with the optimized prompt--------
the United Kingdom. England England England... the United Kingdom


Params:
suffix_length = 10
nb_steps = 1000
lr = 10000

Result:
(venv) membit@coktailjet:~/projects/reverse/reverse_engineering_llms$ python3 continuous_descent.py
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
batch [1524, 19, 8, 1784, 13, 32099, 3, 5]
Initial prompt tokens: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']
Embedding(32100, 768)
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels tokens: ['<extra_id_0>', '▁France', '<extra_id_1>']
100%|██████████████████████████████████████████████████| 1000/1000 [01:17<00:00, 12.93it/s]
prompt_ids[0] tensor([ 1410,  1410,  1410,  1410,  1410,  1410,  1410,  1410,  1410,  1410,
         1524,    19,     8,  1784,    13, 32099,     3,     5,     1],
       device='cuda:0')
Optimized prompt: ['▁France', '▁France', '▁France', '▁France', '▁France', '▁France', '▁France', '▁France', '▁France', '▁France', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
the United Kingdom the United Kingdom the United Kingdom the United Kingdom the United

--------Generated with the optimized prompt--------
England England England England England. London is the capital of England
(venv) membit@coktailjet:~/projects/reverse/reverse_engineering_llms$
















--------------------------- DISCRETE ---------------------------
    suffix_length = 19
    nb_steps = 100
    k = 3

input: ['▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '</s>']
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels: ['<extra_id_0>', '▁France']
100%|████████████████████████████████████████████████| 100/100 [02:55<00:00,  1.76s/it]
Optimized prompt: ['▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁incearca', 'François', '▁proaspat', 'uré', '▁contul', 'Générale', '▁proaspat', 'ôme', '▁varsta', '▁linistit', '▁France', '▁frigider', '▁frumos', '<extra_id_27>', 'OND', '▁“', '▁caci', '▁dimineata', '▁Morning', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
England.

--------Generated with the optimized prompt--------
the British Empire.  the British Empire. the British Empire.




    suffix_length = 20
    nb_steps = 100
    k = 3

input: ['▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '</s>']
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels: ['<extra_id_0>', '▁France']
100%|████████████████████████████████████████████████| 100/100 [03:03<00:00,  1.83s/it]
Optimized prompt: ['▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', 'ographer', '▁concasseur', '▁Guillaume', '▁accesorii', '▁română', 'Gra', '▁Lieutenant', '▁author', 'blanc', '/2019', 'veti', 'gu', '▁galben', '▁incearca', '▁Brig', '▁fisier', '▁trackback', '▁Magn', 'feti', 'FN', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
England.

--------Generated with the optimized prompt--------
France. Bi France. Bi France. Bi France. Bi a




=====================PREFIX

    suffix_length = 5
    nb_steps = 100
    k = 3

input: ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '</s>']
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels: ['<extra_id_0>', '▁France']
100%|████████████████████████████████████████████████████| 100/100 [00:51<00:00,  1.96it/s]
Optimized prompt: ['▁circulation', 'Char', 'rruption', 'haudiere', '▁Alfred', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
England.

--------Generated with the optimized prompt--------
England.



    suffix_length = 6
    nb_steps = 100
    k = 3
  
input: ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '</s>']
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels: ['<extra_id_0>', '▁France']
100%|████████████████████████████████████████████████████| 100/100 [01:00<00:00,  1.65it/s]
Optimized prompt: ['mila', '▁miscare', '▁vizualiz', '▁pamant', '▁Sfant', '▁departe', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
England.

--------Generated with the optimized prompt--------
France.

















======================= Continuous =======================
    suffix_length = 20
    nb_steps = 100
    lr = 0.1
  
Initial prompt tokens: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels tokens: ['<extra_id_0>', '▁France', '<extra_id_1>']
100%|████████████████████████████████████████████████████| 100/100 [00:07<00:00, 13.68it/s]
Optimized prompt: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
the United Kingdom the United Kingdom the United Kingdom the United Kingdom the United

--------Generated with the optimized prompt--------
newoutput tensor([    0, 32099,  2789,     5, 32098,  2789,     5, 32097,  2789,     5,
        32096,  2789,     5, 32095,     5,     5, 32094,  2789,     5, 32093],
       device='cuda:0')
England. England. England. England... England.
out.logits torch.Size([1, 3, 32100])
output from embedding: the



suffix_length = 20
    nb_steps = 100
    lr = 1

Initial prompt tokens: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels tokens: ['<extra_id_0>', '▁France', '<extra_id_1>']
100%|████████████████████████████████████████████████████| 100/100 [00:07<00:00, 13.55it/s]
Optimized prompt: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
the United Kingdom the United Kingdom the United Kingdom the United Kingdom the United

--------Generated with the optimized prompt--------
newoutput tensor([    0, 32099,  2789,     5, 32098,  2789,     5, 32097,  2789,     5,
        32096,  2789,     5, 32095,     5,     5, 32094,  2789,     5, 32093],
       device='cuda:0')
England. England. England. England... England.
out.logits torch.Size([1, 3, 32100])
output from embedding: France




    suffix_length = 20
    nb_steps = 100
    lr = 10

Initial prompt tokens: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels tokens: ['<extra_id_0>', '▁France', '<extra_id_1>']
100%|████████████████████████████████████████████████████| 100/100 [00:07<00:00, 14.08it/s]
Optimized prompt: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
the United Kingdom the United Kingdom the United Kingdom the United Kingdom the United

--------Generated with the optimized prompt--------
newoutput tensor([    0, 32099,  2789,     5, 32098,  2789,     5, 32097,  2789,     5,
        32096,  2789,     5, 32095,     5,     5, 32094,  2789,     5, 32093],
       device='cuda:0')
England. England. England. England... England.
out.logits torch.Size([1, 3, 32100])
output from embedding: France


    suffix_length = 20
    nb_steps = 100
    lr = 100

Initial prompt tokens: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels tokens: ['<extra_id_0>', '▁France', '<extra_id_1>']
100%|████████████████████████████████████████████████████| 100/100 [00:07<00:00, 13.53it/s]
Optimized prompt: ['farbe', 'cativa', '▁2017.', 'Datorita', '▁Audio', '▁dorint', 'nique', '▁interioare', '<extra_id_27>', '▁France', '▁cautare', 'ologi', '.', '▁suprafete', '▁summarize', '▁adica', '▁Nantes', '2.', 'haudiere', 'Biserica', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
the United Kingdom the United Kingdom the United Kingdom the United Kingdom the United

--------Generated with the optimized prompt--------
newoutput tensor([    0, 32099,  2789, 32098,     5,     1], device='cuda:0')
England.
out.logits torch.Size([1, 3, 32100])
output from embedding: France



NOT INTERPRETABLE...
romania, the world, uk


    suffix_length = 20
    nb_steps = 100
    lr = 1000

Initial prompt tokens: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels tokens: ['<extra_id_0>', '▁France', '<extra_id_1>']
100%|████████████████████████████████████████████████████| 100/100 [00:07<00:00, 12.79it/s]
Optimized prompt: ['stunde', '▁ganduri', '<extra_id_27>', 'tră', 'uleiul', '▁cartofi', '▁senzati', '▁gradini', 'stick', 'EAN', '..."', 'Asociația', '▁France', '▁cautare', '▁Popescu', 'illes', '▁Județean', '▁pretul', '▁senzati', '▁foloseste', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
the United Kingdom the United Kingdom the United Kingdom the United Kingdom the United

--------Generated with the optimized prompt--------
newoutput tensor([    0, 32099,  3871, 32098,     5, 32097,  3871, 32096,  3871, 32095,
            5, 32094,  3871,     5, 32093,     5, 32092,     3, 32091,     3],
       device='cuda:0')
Romania. Romania Romania. Romania..
out.logits torch.Size([1, 3, 32100])
output from embedding: France



    suffix_length = 20
    nb_steps = 100
    lr = 10000

Initial prompt tokens: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels tokens: ['<extra_id_0>', '▁France', '<extra_id_1>']
100%|████████████████████████████████████████████████████| 100/100 [00:07<00:00, 12.84it/s]
Optimized prompt: ['▁linguri', '▁primavara', '▁vanzari', '▁județ', 'incepand', '▁cartofi', '▁machiaj', '▁proaspat', 'arie', '▁rosii', 'culoarea', 'ceapa', '▁lucrari', '▁dumneavoastra', 'Provence', '▁Fr', 'nez', 'crumbs', 'oilea', '▁parinti', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
the United Kingdom the United Kingdom the United Kingdom the United Kingdom the United

--------Generated with the optimized prompt--------
newoutput tensor([    0, 32099,  3871,     5,     1], device='cuda:0')
Romania.
out.logits torch.Size([1, 3, 32100])
output from embedding: France



    suffix_length = 20
    nb_steps = 500
    lr = 1000

Initial prompt tokens: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels tokens: ['<extra_id_0>', '▁France', '<extra_id_1>']
100%|████████████████████████████████████████████████████| 500/500 [00:37<00:00, 13.26it/s]
Optimized prompt: ['Exista', '▁ganduri', '▁împlin', '▁achizitiona', '▁fisier', '▁pielii', '▁imunitar', '▁gradini', '▁solutie', 'Produsele', 'rgic', '▁gradini', '▁France', '▁niciodata', '▁răc', '▁zahar', '▁Județean', '▁livrare', 'informatiile', '▁incadr', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
the United Kingdom the United Kingdom the United Kingdom the United Kingdom the United

--------Generated with the optimized prompt--------
newoutput tensor([    0, 32099,     8,   296,     5, 32098,     8,   296,     5,     1],
       device='cuda:0')
the world. the world.
out.logits torch.Size([1, 3, 32100])
output from embedding: France



    suffix_length = 20
    nb_steps = 1000
    lr = 100

Initial prompt tokens: ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
labels tokens: ['<extra_id_0>', '▁France', '<extra_id_1>']
100%|██████████████████████████████████████████████████| 1000/1000 [01:40<00:00,  9.98it/s]
Optimized prompt: ['▁senzati', 'tufted', '▁prevazut', 'Datorita', 'cît', 'cît', 'ouette', 'Asociația', '▁Bisericii', '▁Sfant', '▁senzati', '▁imobil', '▁pielii', 'permalink', '▁summarize', '▁senzati', '▁dispoziti', '▁Preisvergleich', '▁gradini', '▁senzati', '▁London', '▁is', '▁the', '▁capital', '▁of', '<extra_id_0>', '▁', '.', '</s>']

--------Generated with the user input--------
/home/lamsade/membit/projects/reverse/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
the United Kingdom the United Kingdom the United Kingdom the United Kingdom the United

--------Generated with the optimized prompt--------
newoutput tensor([    0, 32099,     8,  1270,     5,     1], device='cuda:0')
the UK.
out.logits torch.Size([1, 3, 32100])
output from embedding: France























