# LLM Attacks

Code of the paper ["Universal and Transferable Adversarial Attacks on Aligned Language Models"](https://arxiv.org/abs/2307.15043).

It implements the Greedy Coordinate Gradient method.

You need to install the requirements by running ```pip install -r requirements.txt``` in the ```paper/gcg/llm-attacks``` folder.

I added ```gcg.py``` which is the python script used to compute the success rate of the individual harmful behaviors experiment. You need to run it from the ```paper``` folder.
