import torch
import wandb

def generate(model, tokenizer, input_ids, assistant_role_slice, gen_config=None):
    if gen_config is None:
        gen_config = model.generation_config
        gen_config.max_new_tokens = 32

    if gen_config.max_new_tokens > 50:
        print('WARNING: max_new_tokens > 32 may cause testing to slow down.')
        
    input_ids = input_ids[:assistant_role_slice.stop].to(model.device).unsqueeze(0)
    attn_masks = torch.ones_like(input_ids).to(model.device)
    output_ids = model.generate(input_ids, 
                                attention_mask=attn_masks, 
                                generation_config=gen_config,
                                pad_token_id=tokenizer.pad_token_id)[0]

    return output_ids[assistant_role_slice.stop:]

def check_for_attack_success(model, tokenizer, input_ids, assistant_role_slice, test_prefixes, gen_config=None):
    gen_str = tokenizer.decode(generate(model, 
                                        tokenizer, 
                                        input_ids, 
                                        assistant_role_slice, 
                                        gen_config=gen_config)).strip()
    jailbroken = not any([prefix in gen_str for prefix in test_prefixes])
    return jailbroken

def check_for_attack_success_embeds(model, tokenizer, embeds, assistant_role_slice, test_prefixes, suffix_manager, gen_config=None):
    generation = model.generate(inputs_embeds=embeds[:,:suffix_manager._target_slice.start,:], max_length=128)
    gen_str = tokenizer.decode(generation[0].cpu().numpy(), skip_special_tokens=True)
    print("gen_str:", gen_str)
    jailbroken = not any([prefix in gen_str for prefix in test_prefixes])
    return jailbroken

def print_k_first_tokens(k, distributions, tokenizer):
    # print the k most probable tokens
    for distrib in distributions:
        topk_indices = torch.topk(distrib, k).indices
        topk_probs = torch.topk(distrib, k).values
        topk_tokens = tokenizer.convert_ids_to_tokens(topk_indices)
        for tok, prob in zip(topk_tokens, topk_probs):
            print(f"{tok} ({prob:.2f})", end=" ")
        print()

def mean_distance_to_embedding_matrix(embeddings, embedding_matrix, metric="cosine", matrix=False):
    # returns the mean of cosine sim between each embedding and their closest (cosine) embedding in the embedding matrix
    # embeddings.shape: [nb_embeddings, embedding_dim]
    # embedding_matrix.shape: [vocab_size, embedding_dim]
    smallest_value = torch.finfo(embedding_matrix.dtype).min
    highest_value = torch.finfo(embedding_matrix.dtype).max
    dot_product = torch.matmul(embeddings, embedding_matrix.T) # [nb_embeddings, vocab_size]
    if metric == "cosine":
        denom = torch.matmul(torch.norm(embeddings, dim=-1, keepdim=True), torch.norm(embedding_matrix, dim=-1, keepdim=True).T)
        cosine_sim = dot_product/denom # [nb_embeddings, vocab_size]
        if matrix:
            cosine_sim.fill_diagonal_(smallest_value) # to avoid the same embedding
        maxs = torch.max(cosine_sim, dim=-1)
        maxs_values = maxs.values # [nb_embeddings]
        maxs_indices = maxs.indices # [nb_embeddings]
        closest_embeddings = embedding_matrix[maxs_indices] # [nb_embeddings, embedding_dim]
        return closest_embeddings, maxs_values
    elif metric == "dot product":
        if matrix:
            dot_product.fill_diagonal_(smallest_value) # to avoid the same embedding
        maxs = torch.max(dot_product, dim=-1)
        maxs_values = maxs.values
        maxs_indices = maxs.indices
        closest_embeddings = embedding_matrix[maxs_indices]
        return closest_embeddings, maxs_values
    elif metric == "L2":
        x_norm = torch.norm(embeddings, p=2, dim=-1) # [nb_embeddings]
        x_norm_repeated = x_norm.unsqueeze(1).repeat(1, embedding_matrix.shape[0]) # [nb_embeddings, vocab_size]
        w_norm = torch.norm(embedding_matrix, p=2, dim=-1) # [vocab_size]
        w_norm_repeated = w_norm.unsqueeze(0).repeat(embeddings.shape[0], 1) # [nb_embeddings, vocab_size]
        L2_dist = torch.sqrt(x_norm_repeated**2 + w_norm_repeated**2 - 2*dot_product) # [nb_embeddings, vocab_size]
        if matrix:
            L2_dist.fill_diagonal_(highest_value)
        min_L2 = torch.min(L2_dist, dim=-1)
        min_L2_values = min_L2.values # [nb_embeddings]
        min_L2_indices = min_L2.indices # [nb_embeddings]
        closest_embeddings = embedding_matrix[min_L2_indices] # [nb_embeddings, embedding_dim]
        return closest_embeddings, min_L2_values

def distance_matrix_to_matrix(embedding_matrix, metric="cosine"):
    # (to check if the embeddings go far away from the embedding matrix)
    # returns the mean of cosine sim between each embedding of the embedding matrix and their closest (cosine) embedding in the embedding matrix
    # embedding_matrix.shape: [vocab_size, embedding_dim]
    return mean_distance_to_embedding_matrix(embedding_matrix, embedding_matrix, metric=metric, matrix=True)

def distance_to_embedding_matrix(embedding, embedding_matrix):
    return mean_distance_to_embedding_matrix(embedding.unsqueeze(0), embedding_matrix)

def log_statistics(adv_embeddings, model):
    """
    adv_embeddings: tensor of shape [nb_embeddings, embedding_dim]

    Compute useful statistics to understand the embedding space
    - norm of the embeddings
    - cosine with the closest embedding in the embedding matrix
    - norm of the closest embedding in the embedding matrix
    - mean distance to the embedding matrix
    """
    embedding_matrix = model.get_input_embeddings().weight
    norms = torch.norm(adv_embeddings, p=2, dim=-1)
    wandb.log({"iterate norms": norms.mean().item()})
    metrics = ["cosine", "dot product", "L2"]
    for metric in metrics:
        closest_embeddings, values = mean_distance_to_embedding_matrix(adv_embeddings, embedding_matrix, metric=metric)
        closest_norms = torch.norm(closest_embeddings, p=2, dim=-1)
        wandb.log({f"iterate {metric} with closest embedding": values.mean().item(),
                   f"iterate {metric} closest embedding norms": closest_norms.mean().item()})
