"""
My attack decomposes in two steps:
1. Continuous optimization of the input
2. Find a discrete prompt that behaves like the continuous input
"""

import gc

import numpy as np
import torch
import torch.nn as nn
from tqdm.auto import tqdm
import torch.nn.functional as F

from llm_attacks import AttackPrompt, MultiPromptAttack, PromptManager
from llm_attacks import get_embedding_matrix, get_embeddings

def output_entropy(probabilities, dim=-1, epsilon=1e-3):
    """
    Computes the entropy of a probability distribution along a specified dimension.

    Args:
        probabilities (torch.Tensor): Tensor containing the probability distribution.
        dim (int): Dimension along which to compute the entropy.
        epsilon (float): Small constant to avoid log(0) and division by zero.

    Returns:
        torch.Tensor: Entropy computed along the specified dimension.
    """
    # Step 1: Clamp probabilities to avoid log(0) and ensure numerical stability
    probabilities = torch.clamp(probabilities, min=epsilon, max=1.0)

    # Step 2: Normalize the probabilities along the specified dimension
    probabilities_sum = probabilities.sum(dim=dim, keepdim=True)
    probabilities = probabilities / probabilities_sum

    # Step 3: Compute entropy along the specified dimension
    entropy = -torch.sum(probabilities * torch.log(probabilities), dim=dim)

    return entropy

def entropy_attraction(prompt_embeddings, model, metric='dot product'):
    embedding_matrix = model.get_input_embeddings().weight # [vocab_size, embedding_dim]
    # prompt_embeddings.shape = [1, seq_len, embedding_dim]
    dot_product = torch.matmul(prompt_embeddings.squeeze(0), embedding_matrix.T)
    if metric == 'dot product':
        softmax_dot_product = F.softmax(dot_product, dim=-1) # [seq_len, vocab_size]
        entropies = -torch.sum(softmax_dot_product * torch.log(softmax_dot_product), dim=-1)
        #print("manual attraction entropy:", entropies.mean())
        #entropies = torch.stack([torch.distributions.Categorical(probs=softmax_dot_product[i]).entropy() for i in range(softmax_dot_product.shape[0])])
        entropy = entropies.mean()
        #print("torch attraction entropy:", entropy)
        return entropy
    elif metric == 'cosine':
        nb_embeddings = prompt_embeddings.shape[0]
        vocab_size = embedding_matrix.shape[0]

        emb_norms = torch.norm(prompt_embeddings, dim=-1, keepdim=True)
        emb_norms = emb_norms.repeat(1, vocab_size)
        mat_norms = torch.norm(embedding_matrix, dim=-1, keepdim=True).T
        mat_norms = mat_norms.repeat(nb_embeddings, 1)
        norms = emb_norms*mat_norms
        cosine_similarity = dot_product / norms
        softmax_dot_product = F.softmax(cosine_similarity, dim=-1)
        entropies = -torch.sum(softmax_dot_product * torch.log(softmax_dot_product), dim=-1)
        entropy = entropies.mean()
        return entropy
    
def distance_to_closest_embedding(embeddings, model, metric='cosine'):
    # rather that softmaxing and taking the entropy we return the distance to the closest embedding
    embedding_matrix = model.get_input_embeddings().weight
    dot_product = torch.matmul(embeddings.squeeze(0), embedding_matrix.T) # [seq_len, vocab_size]
    if metric == 'dot product':
        max_dot_products = torch.max(dot_product, dim=-1).values
        return -max_dot_products.mean() # minus because we will minimize it
    elif metric == 'cosine':
        nb_embeddings = embeddings.shape[0]
        vocab_size = embedding_matrix.shape[0]
        emb_norms = torch.norm(embeddings, dim=-1, keepdim=True)
        emb_norms = emb_norms.repeat(1, vocab_size)
        mat_norms = torch.norm(embedding_matrix, dim=-1, keepdim=True).T
        mat_norms = mat_norms.repeat(nb_embeddings, 1)
        norms = emb_norms*mat_norms
        cosine_similarity = dot_product / norms
        max_cosine = torch.max(cosine_similarity, dim=-1).values
        return -max_cosine.mean() # minus because we will minimize it

def log_likelihood(embeddings_sentence, model, tokenizer, epsilon=1e-3):
    # embeddings_sentence.shape = [batch_size, seq_len, embedding_dim]

    # we add the bos token and remove the last token of the input
    batch_size = embeddings_sentence.shape[0]
    bos_embedding = model.get_input_embeddings().weight[tokenizer.bos_token_id].unsqueeze(0).unsqueeze(0) # [1, 1, embedding_dim]
    bos_embedding = bos_embedding.repeat(batch_size, 1, 1) # [batch_size, 1, embedding_dim]
    modified_embeddings_sentence = torch.cat((bos_embedding, embeddings_sentence[:, :-1, :]), dim=1) # [batch_size, seq_len, embedding_dim]

    # we run it
    outputs = model(inputs_embeds=modified_embeddings_sentence)
    probabilities = torch.softmax(outputs.logits, dim=-1) # [batch_size, seq_len, vocab_size]

    embedding_matrix = model.get_input_embeddings().weight # [vocab_size, embedding_dim]
    embedding_matrix = embedding_matrix.repeat(batch_size, 1, 1) # [batch_size, vocab_size, embedding_dim]
    WEx = torch.bmm(embedding_matrix, embeddings_sentence.transpose(1, 2)) # [batch_size, vocab_size, seq_len]
    
    # indices of the closest embeddings in the embedding matrix
    indices = torch.argmax(WEx, dim=1) # [batch_size, seq_len]

    # gather along the last dimension (vocab_size)
    cond_prob = torch.gather(input=probabilities, dim=-1, index=indices.unsqueeze(-1)).squeeze(-1) # [batch_size, seq_len]
    cond_prob = torch.clamp(cond_prob, min=epsilon, max=1.0) # to prevent log(0)
    
    log_likelihood = torch.sum(torch.log(cond_prob), dim=-1)
    return log_likelihood # [batch_size]

def compute_loss(model, tokenizer, input_embeds, attention_mask, input_ids, control_slices, target_slices, loss_slices, w_ce, w_at, w_nl, w_en):
    #logits = model(input_ids, attention_mask, control_slices).logits
    logits = model(input_ids=input_ids, attention_mask=attention_mask).logits

    # CE loss to generate target
    ce_loss_fn = nn.CrossEntropyLoss()
    ce_losses = [ce_loss_fn(logits[i,loss_slices[i],:], input_ids[i,target_slices[i]]) for i in range(logits.shape[0])]
    ce_losses = torch.stack(ce_losses)
    ce_loss = ce_losses.mean()

    """
    # attraction loss to keep the embeddings close to the embedding matrix
    attractions = entropy_attraction(input_embeds[:,input_slice,:], model)
    attraction = attractions.mean()

    # negative log likelihood to keep the prompt likely
    nlls = -log_likelihood(input_embeds, model, tokenizer)
    nll = nlls.mean()

    # entropy regularization to add information
    distribs = torch.softmax(logits[:,loss_slice,:], dim=-1) # shape: (batch_size, seq_len, vocab_size)
    entropies = output_entropy(distribs, dim=-1).sum()
    entropy = entropies.mean()
    """

    # final loss
    loss = w_ce*ce_loss # + w_at*attraction + w_nl*nll - w_en*entropy
    
    return loss

# ORIGNAL LOSS
def compute_peft_loss(input_batch, labels, model, tokenizer, device):
    # return -log(p(label|input))
    batch_size = len(input_batch)
    inputs = [input_batch[i] + labels[i] for i in range(batch_size)]
    input_ids = tokenizer(inputs, return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)
    label_ids = tokenizer(labels, return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)
    label_length = label_ids.shape[1]
    outputs = model(input_ids=input_ids, labels=input_ids) # doesn't matter, but must be the same size
    distributions = F.softmax(outputs.logits, dim=-1) # [batch_size, seq_len, vocab_size], seq_len = n+m
    selected_distributions = distributions[:, -(label_length+1):-1, :] # [batch_size, m, vocab_size]
    # for each batch, for each token in the prompt, we want to get the probability of the corresponding token in the label
    probabilities = torch.zeros((batch_size, label_length)).to(device)
    for batch in range(batch_size):
        for i in range(label_length):
            probabilities[batch, i] = selected_distributions[batch, i, label_ids[batch, i]]
    loss = -torch.sum(torch.log(probabilities), dim=-1).mean() # mean over the batch
    return loss

# ATTRACTION ENTROPY LOSS TERM
def compute_peft_attraction(prompt_embeddings, model):
    embedding_matrix = model.get_input_embeddings().weight
    dot_product = torch.matmul(prompt_embeddings.squeeze(0), embedding_matrix.T)
    softmax_dot_product = F.softmax(dot_product, dim=-1)
    entropy = -torch.sum(softmax_dot_product * torch.log(softmax_dot_product), dim=-1).mean()
    return entropy

def embedding_gradients(model, tokenizer, input_ids, input_embeds, input_slice, target_slice, loss_slice, w_ce, w_at, w_nl, w_en):

    """
    Computes gradients of the loss with respect to the embeddings of the input.
    
    Parameters
    ----------
    model : Transformer Model
        The transformer model to be used.
    tokenizer : PreTrainedTokenizer
        The tokenizer to be used.
    input_ids : torch.Tensor
        The input sequence in the form of token ids.
    input_embeds : torch.Tensor
        The input sequence in the form of embeddings.
    input_slice : slice
        The slice of the input sequence for which gradients need to be computed.
    target_slice : slice
        The slice of the input sequence to be used as targets.
    loss_slice : slice
        The slice of the logits to be used for computing the loss.
    w_ce : float
        Weight of the cross-entropy loss.
    w_at : float
        Weight of the attraction loss.
    w_nl : float
        Weight of the negative log likelihood loss.
    w_en : float
        Weight of the entropy regularization.

    Returns
    -------
    torch.Tensor
        The gradients of each embedding in the input_slice with respect to the loss.
    """

    input_embeds.requires_grad_()
    input_embeds.grad = None

    logits = model(inputs_embeds=input_embeds).logits
    #print("logits[:,loss_slice,:].shape:", logits[:,loss_slice,:].shape)
    targets = input_ids[:,target_slice]
    #print("targets.shape:", targets.shape)

    # CE loss to generate target
    ce_loss_fn = nn.CrossEntropyLoss()
    ce_losses = [ce_loss_fn(logits[i,loss_slice,:], targets[i]) for i in range(logits.shape[0])]
    ce_losses = torch.stack(ce_losses)
    #print("ce_losses:", ce_losses.shape)
    ce_loss = ce_losses.mean()
    #print("ce_loss:", ce_loss.shape)

    # attraction loss to keep the embeddings close to the embedding matrix
    #attraction = entropy_attraction(input_embeds[0,input_slice,:], model)
    attraction = distance_to_closest_embedding(input_embeds[0,input_slice,:], model, metric='cosine')

    # negative log likelihood to keep the prompt likely
    nll = -log_likelihood(input_embeds, model, tokenizer)[0]

    # entropy regularization to add information
    distribs = torch.softmax(logits[0,loss_slice,:], dim=-1) # shape: (seq_len, vocab_size)
    entropy = output_entropy(distribs, dim=-1).sum()

    # final loss
    loss = w_ce*ce_loss + w_at*attraction + w_nl*nll - w_en*entropy

    input_embeds.retain_grad()
    loss.backward(retain_graph=True)

    # input slice
    #print("input_slice:", tokenizer.decode(input_ids[input_slice].tolist()))

    grad = input_embeds.grad[:, input_slice, :].clone()
    grad = grad / grad.norm(dim=-1, keepdim=True)

    input_embeds.grad.zero_()
    model.zero_grad()
    #input_embeds.grad = None
    
    return grad, loss.item(), ce_loss.item(), attraction.item(), nll.item(), entropy.item()

class MyAttackPrompt(AttackPrompt):

    def __init__(self, *args, **kwargs):
        
        super().__init__(*args, **kwargs)
    
    def grad(self, model):
        return embedding_gradients(
            model, 
            self.input_ids.to(model.device), 
            self._control_slice, 
            self._target_slice, 
            self._loss_slice
        )
